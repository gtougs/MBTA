#!/usr/bin/env python3
"""
MBTA Transit Analytics Dashboard - Streamlit Application

This Streamlit application provides a comprehensive view of the MBTA transit data
engineering pipeline, including real-time analytics, historical trends, and detailed
documentation of the system architecture and design decisions.

Author: MBTA Data Engineering Team
Version: 1.0.0
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import asyncio
import sys
from pathlib import Path
from datetime import datetime, timedelta
import json

# Add src to Python path for imports
src_path = Path(__file__).parent / "src"
sys.path.insert(0, str(src_path))

# Import MBTA pipeline modules
try:
    from mbta_pipeline.storage.database import DatabaseManager
    from mbta_pipeline.processing.analytics import transit_analytics
    from mbta_pipeline.processing.analytics_queries import AnalyticsQueries
except ImportError as e:
    st.error(f"Failed to import MBTA pipeline modules: {e}")
    st.info("Please ensure the MBTA pipeline is properly installed and configured.")
    st.stop()

# Page configuration
st.set_page_config(
    page_title="MBTA Transit Analytics Dashboard",
    page_icon="ğŸš‡",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better styling
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .section-header {
        font-size: 2rem;
        font-weight: bold;
        color: #2c3e50;
        margin-top: 2rem;
        margin-bottom: 1rem;
        border-bottom: 3px solid #3498db;
        padding-bottom: 0.5rem;
    }
    .metric-card {
        background-color: #f8f9fa;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #3498db;
        margin: 0.5rem 0;
    }
    .info-box {
        background-color: #e8f4fd;
        border: 1px solid #3498db;
        border-radius: 0.5rem;
        padding: 1rem;
        margin: 1rem 0;
    }
    .warning-box {
        background-color: #fff3cd;
        border: 1px solid #ffc107;
        border-radius: 0.5rem;
        padding: 1rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# Initialize database connection
@st.cache_resource
def get_database_manager():
    """Get database manager instance with caching."""
    try:
        return DatabaseManager()
    except Exception as e:
        st.error(f"Failed to initialize database: {e}")
        return None

# Data loading functions
@st.cache_data(ttl=300)  # Cache for 5 minutes
def load_dashboard_overview():
    """Load dashboard overview data."""
    try:
        db_manager = get_database_manager()
        if not db_manager:
            return None
        
        # Get basic statistics
        session = db_manager.get_session()
        try:
            # Count records in each table
            tables = ['routes', 'stops', 'trips', 'predictions', 'vehicle_positions', 'trip_updates', 'alerts']
            counts = {}
            for table in tables:
                result = session.execute(f"SELECT COUNT(*) FROM {table}")
                counts[table] = result.scalar()
            
            # Get recent predictions
            recent_preds = session.execute("""
                SELECT COUNT(*) FROM predictions 
                WHERE timestamp >= NOW() - INTERVAL '1 hour'
            """).scalar()
            
            return {
                'table_counts': counts,
                'recent_predictions': recent_preds,
                'last_updated': datetime.now()
            }
        finally:
            session.close()
    except Exception as e:
        st.error(f"Error loading dashboard overview: {e}")
        return None

@st.cache_data(ttl=300)
def load_route_performance(hours=24):
    """Load route performance data."""
    try:
        db_manager = get_database_manager()
        if not db_manager:
            return None
        
        session = db_manager.get_session()
        try:
            query = """
                SELECT 
                    r.id as route_id,
                    r.route_name,
                    COUNT(p.id) as total_predictions,
                    COUNT(CASE WHEN p.delay > 0 THEN 1 END) as delayed_predictions,
                    ROUND(
                        CASE 
                            WHEN COUNT(p.id) > 0 THEN 
                                ((COUNT(p.id) - COUNT(CASE WHEN p.delay > 0 THEN 1 END))::numeric / COUNT(p.id)) * 100
                            ELSE 0 
                        END, 2
                    ) as on_time_percentage,
                    ROUND(
                        CASE 
                            WHEN COUNT(CASE WHEN p.delay > 0 THEN 1 END) > 0 THEN 
                                (AVG(CASE WHEN p.delay > 0 THEN p.delay ELSE 0 END) / 60.0)::numeric
                            ELSE 0 
                        END, 2
                    ) as avg_delay_minutes
                FROM routes r
                LEFT JOIN predictions p ON r.id = p.route_id 
                    AND p.timestamp >= NOW() - INTERVAL '%s hours'
                GROUP BY r.id, r.route_name
                ORDER BY on_time_percentage DESC NULLS LAST
            """ % hours
            
            result = session.execute(query)
            df = pd.DataFrame(result.fetchall(), columns=result.keys())
            return df
        finally:
            session.close()
    except Exception as e:
        st.error(f"Error loading route performance: {e}")
        return None

@st.cache_data(ttl=300)
def load_recent_alerts(hours=24):
    """Load recent alerts data."""
    try:
        db_manager = get_database_manager()
        if not db_manager:
            return None
        
        session = db_manager.get_session()
        try:
            query = """
                SELECT 
                    alert_id,
                    alert_header_text,
                    alert_description_text,
                    alert_severity_level,
                    effective_start_date,
                    effective_end_date,
                    affected_routes,
                    timestamp
                FROM alerts 
                WHERE timestamp >= NOW() - INTERVAL '%s hours'
                ORDER BY timestamp DESC
                LIMIT 20
            """ % hours
            
            result = session.execute(query)
            df = pd.DataFrame(result.fetchall(), columns=result.keys())
            return df
        finally:
            session.close()
    except Exception as e:
        st.error(f"Error loading alerts: {e}")
        return None

# Main dashboard function
def main():
    """Main dashboard function."""
    
    # Header
    st.markdown('<h1 class="main-header">MBTA Transit Analytics Dashboard</h1>', unsafe_allow_html=True)
    
    # Sidebar navigation
    st.sidebar.title("Navigation")
    page = st.sidebar.selectbox(
        "Select a page:",
        ["Overview", "Route Performance", "Alerts & Service", "Data Pipeline Documentation", "System Architecture"]
    )
    
    # Load data
    overview_data = load_dashboard_overview()
    
    if page == "Overview":
        show_overview_page(overview_data)
    elif page == "Route Performance":
        show_route_performance_page()
    elif page == "Alerts & Service":
        show_alerts_page()
    elif page == "Data Pipeline Documentation":
        show_documentation_page()
    elif page == "System Architecture":
        show_architecture_page()

def show_overview_page(overview_data):
    """Display the overview page."""
    st.markdown('<h2 class="section-header">System Overview</h2>', unsafe_allow_html=True)
    
    if not overview_data:
        st.warning("Unable to load dashboard data. Please check the database connection.")
        return
    
    # Key metrics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            label="Total Routes",
            value=overview_data['table_counts'].get('routes', 0),
            help="Number of transit routes in the system"
        )
    
    with col2:
        st.metric(
            label="Total Stops",
            value=overview_data['table_counts'].get('stops', 0),
            help="Number of transit stops in the system"
        )
    
    with col3:
        st.metric(
            label="Total Predictions",
            value=overview_data['table_counts'].get('predictions', 0),
            help="Total number of arrival/departure predictions"
        )
    
    with col4:
        st.metric(
            label="Recent Predictions (1h)",
            value=overview_data['recent_predictions'],
            help="Predictions received in the last hour"
        )
    
    # Data freshness indicator
    st.markdown('<h3>Data Freshness</h3>', unsafe_allow_html=True)
    
    last_updated = overview_data['last_updated']
    time_diff = datetime.now() - last_updated
    
    if time_diff.total_seconds() < 300:  # Less than 5 minutes
        status_color = "green"
        status_text = "ğŸŸ¢ Real-time data flowing"
    elif time_diff.total_seconds() < 1800:  # Less than 30 minutes
        status_color = "orange"
        status_text = "ğŸŸ¡ Data may be stale"
    else:
        status_color = "red"
        status_text = "ğŸ”´ Data is stale"
    
    st.markdown(f"""
    <div class="metric-card">
        <strong>Last Data Update:</strong> {last_updated.strftime('%Y-%m-%d %H:%M:%S UTC')}<br>
        <strong>Status:</strong> <span style="color: {status_color};">{status_text}</span>
    </div>
    """, unsafe_allow_html=True)
    
    # System health
    st.markdown('<h3>System Health</h3>', unsafe_allow_html=True)
    
    # Check if pipeline is running (simplified check)
    try:
        db_manager = get_database_manager()
        if db_manager and db_manager.test_connection():
            st.success("âœ… Database connection healthy")
        else:
            st.error("âŒ Database connection failed")
    except:
        st.error("âŒ Database connection failed")
    
    # Data pipeline status
    if overview_data['recent_predictions'] > 0:
        st.success("âœ… Data pipeline is actively collecting data")
    else:
        st.warning("âš ï¸ No recent data detected. Pipeline may be idle.")

def show_route_performance_page():
    """Display the route performance page."""
    st.markdown('<h2 class="section-header">Route Performance Analytics</h2>', unsafe_allow_html=True)
    
    # Time range selector
    col1, col2 = st.columns([1, 3])
    with col1:
        hours = st.selectbox("Time Range:", [1, 6, 12, 24, 48, 72], index=3)
    
    with col2:
        st.info(f"Showing performance data for the last {hours} hours")
    
    # Load route performance data
    route_data = load_route_performance(hours)
    
    if route_data is None or route_data.empty:
        st.warning("No route performance data available.")
        return
    
    # Performance metrics
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown('<h3>On-Time Performance</h3>', unsafe_allow_html=True)
        
        # Filter routes with data
        routes_with_data = route_data[route_data['total_predictions'] > 0]
        
        if not routes_with_data.empty:
            fig = px.bar(
                routes_with_data,
                x='route_name',
                y='on_time_percentage',
                title=f'On-Time Performance (Last {hours} Hours)',
                labels={'on_time_percentage': 'On-Time Percentage (%)', 'route_name': 'Route'},
                color='on_time_percentage',
                color_continuous_scale='RdYlGn'
            )
            fig.update_layout(xaxis_tickangle=-45)
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("No route performance data available for the selected time range.")
    
    with col2:
        st.markdown('<h3>Average Delay by Route</h3>', unsafe_allow_html=True)
        
        if not routes_with_data.empty:
            fig = px.bar(
                routes_with_data,
                x='route_name',
                y='avg_delay_minutes',
                title=f'Average Delay by Route (Last {hours} Hours)',
                labels={'avg_delay_minutes': 'Average Delay (Minutes)', 'route_name': 'Route'},
                color='avg_delay_minutes',
                color_continuous_scale='Reds'
            )
            fig.update_layout(xaxis_tickangle=-45)
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("No delay data available for the selected time range.")
    
    # Detailed performance table
    st.markdown('<h3>Detailed Performance Metrics</h3>', unsafe_allow_html=True)
    
    # Format the data for display
    display_data = route_data.copy()
    display_data['total_predictions'] = display_data['total_predictions'].fillna(0).astype(int)
    display_data['delayed_predictions'] = display_data['delayed_predictions'].fillna(0).astype(int)
    display_data['on_time_percentage'] = display_data['on_time_percentage'].fillna(0)
    display_data['avg_delay_minutes'] = display_data['avg_delay_minutes'].fillna(0)
    
    st.dataframe(
        display_data,
        use_container_width=True,
        hide_index=True
    )

def show_alerts_page():
    """Display the alerts and service page."""
    st.markdown('<h2 class="section-header">Service Alerts & Status</h2>', unsafe_allow_html=True)
    
    # Time range selector
    col1, col2 = st.columns([1, 3])
    with col1:
        hours = st.selectbox("Alert Time Range:", [1, 6, 12, 24, 48, 72], index=3, key="alerts")
    
    with col2:
        st.info(f"Showing alerts from the last {hours} hours")
    
    # Load alerts data
    alerts_data = load_recent_alerts(hours)
    
    if alerts_data is None or alerts_data.empty:
        st.info("No service alerts in the selected time range.")
        return
    
    # Alert summary
    col1, col2, col3 = st.columns(3)
    
    with col1:
        total_alerts = len(alerts_data)
        st.metric("Total Alerts", total_alerts)
    
    with col2:
        if 'alert_severity_level' in alerts_data.columns:
            severe_alerts = len(alerts_data[alerts_data['alert_severity_level'] == 'SEVERE'])
            st.metric("Severe Alerts", severe_alerts)
        else:
            st.metric("Severe Alerts", "N/A")
    
    with col3:
        if 'affected_routes' in alerts_data.columns:
            routes_affected = alerts_data['affected_routes'].notna().sum()
            st.metric("Routes Affected", routes_affected)
        else:
            st.metric("Routes Affected", "N/A")
    
    # Alerts table
    st.markdown('<h3>Recent Service Alerts</h3>', unsafe_allow_html=True)
    
    # Format alerts for display
    if not alerts_data.empty:
        # Select columns to display
        display_columns = ['alert_header_text', 'alert_severity_level', 'timestamp']
        if 'alert_description_text' in alerts_data.columns:
            display_columns.append('alert_description_text')
        
        display_data = alerts_data[display_columns].copy()
        
        # Rename columns for better display
        display_data.columns = ['Header', 'Severity', 'Timestamp']
        if 'alert_description_text' in alerts_data.columns:
            display_data.columns = ['Header', 'Severity', 'Timestamp', 'Description']
        
        # Format timestamp
        if 'timestamp' in display_data.columns:
            display_data['Timestamp'] = pd.to_datetime(display_data['Timestamp']).dt.strftime('%Y-%m-%d %H:%M')
        
        st.dataframe(
            display_data,
            use_container_width=True,
            hide_index=True
        )
    else:
        st.info("No alerts to display.")

def show_documentation_page():
    """Display the data pipeline documentation page."""
    st.markdown('<h2 class="section-header">Data Engineering Pipeline Documentation</h2>', unsafe_allow_html=True)
    
    # Overview
    st.markdown("""
    ## System Overview
    
    The MBTA Transit Analytics Dashboard is powered by a robust data engineering pipeline that continuously 
    collects, processes, and analyzes real-time transit data from the Massachusetts Bay Transportation Authority (MBTA).
    
    ### Key Components
    
    1. **Data Ingestion Layer** - Collects data from MBTA APIs
    2. **Data Processing Layer** - Transforms and enriches raw data
    3. **Data Storage Layer** - PostgreSQL database for structured storage
    4. **Analytics Layer** - Real-time calculations and insights
    5. **Visualization Layer** - Interactive dashboards and reports
    """)
    
    # Data Sources
    st.markdown("""
    ## Data Sources
    
    ### MBTA V3 REST API
    - **Endpoint**: https://api-v3.mbta.com
    - **Data Types**: Routes, stops, trips, predictions, vehicles
    - **Update Frequency**: Real-time (every 15 seconds)
    - **Authentication**: API key required
    
    ### MBTA GTFS-RT Feeds
    - **Vehicle Positions**: Real-time location updates
    - **Trip Updates**: Service changes and delays
    - **Service Alerts**: Disruptions and notifications
    - **Update Frequency**: Real-time (every 15 seconds)
    """)
    
    # Data Flow
    st.markdown("""
    ## Data Flow Architecture
    
    ```
    MBTA APIs â†’ Data Ingestion â†’ Data Transformation â†’ Data Storage â†’ Analytics â†’ Dashboard
         â†“              â†“              â†“              â†“           â†“         â†“
    Real-time      Rate-limited    Model-based    PostgreSQL   SQL       Streamlit
    streaming      collection      conversion     storage      queries   interface
    ```
    
    ### 1. Data Ingestion
    - **Rate Limiting**: 1000 requests/minute with burst handling
    - **Error Handling**: Exponential backoff with retry logic
    - **Data Validation**: Schema validation and quality checks
    
    ### 2. Data Transformation
    - **Model Conversion**: Raw API responses â†’ Pydantic models
    - **Data Enrichment**: Add metadata and derived fields
    - **Type Safety**: Strong typing for data consistency
    
    ### 3. Data Storage
    - **Database**: PostgreSQL with optimized schemas
    - **Indexing**: Performance-optimized database indexes
    - **Partitioning**: Time-based data partitioning for scalability
    """)
    
    # Technical Decisions
    st.markdown("""
    ## Technical Design Decisions
    
    ### Why PostgreSQL?
    - **ACID Compliance**: Ensures data integrity for transit operations
    - **JSON Support**: Native JSONB for flexible data storage
    - **Performance**: Excellent query performance with proper indexing
    - **Scalability**: Handles large datasets and concurrent access
    
    ### Why Python Async?
    - **I/O Efficiency**: Non-blocking operations for API calls
    - **Scalability**: Handle multiple concurrent data streams
    - **Ecosystem**: Rich libraries for data processing and ML
    
    ### Why Pydantic Models?
    - **Data Validation**: Automatic schema validation
    - **Type Safety**: Compile-time error checking
    - **Serialization**: Easy JSON conversion for APIs
    - **Documentation**: Self-documenting data structures
    """)
    
    # Data Quality
    st.markdown("""
    ## Data Quality & Reliability
    
    ### Data Validation
    - **Schema Validation**: Ensures data conforms to expected structure
    - **Business Rules**: Validates transit-specific logic
    - **Error Handling**: Graceful degradation for data issues
    
    ### Monitoring & Alerting
    - **Health Checks**: Continuous monitoring of pipeline health
    - **Error Tracking**: Comprehensive error logging and alerting
    - **Performance Metrics**: Real-time performance monitoring
    
    ### Data Freshness
    - **Real-time Updates**: Data updated every 15 seconds
    - **Latency Monitoring**: Track data pipeline latency
    - **Fallback Mechanisms**: Handle API outages gracefully
    """)

def show_architecture_page():
    """Display the system architecture page."""
    st.markdown('<h2 class="section-header">System Architecture & Infrastructure</h2>', unsafe_allow_html=True)
    
    # Architecture Diagram
    st.markdown("""
    ## High-Level Architecture
    
    ```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   MBTA APIs     â”‚    â”‚  GTFS-RT Feeds  â”‚    â”‚  External Data  â”‚
    â”‚   (REST/V3)     â”‚    â”‚   (Real-time)   â”‚    â”‚   Sources       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                      â”‚                      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                      â”‚                      â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Data          â”‚    â”‚   Data          â”‚    â”‚   Data          â”‚
    â”‚   Ingestion     â”‚    â”‚   Processing    â”‚    â”‚   Storage       â”‚
    â”‚   Layer         â”‚    â”‚   Layer         â”‚    â”‚   Layer         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                      â”‚                      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                      â”‚                      â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Analytics     â”‚    â”‚   API Layer     â”‚    â”‚   Dashboard     â”‚
    â”‚   Engine        â”‚    â”‚   (FastAPI)     â”‚    â”‚   (Streamlit)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    ```
    """)
    
    # Component Details
    st.markdown("""
    ## Component Details
    
    ### Data Ingestion Layer
    - **V3 REST Ingestor**: Handles MBTA V3 API calls with rate limiting
    - **GTFS-RT Ingestor**: Processes real-time transit feeds
    - **Connection Pooling**: Efficient HTTP connection management
    - **Error Recovery**: Automatic retry with exponential backoff
    
    ### Data Processing Layer
    - **Data Transformer**: Converts raw API responses to structured models
    - **Data Aggregator**: Real-time aggregation and statistics
    - **Data Validator**: Ensures data quality and consistency
    - **Data Enricher**: Adds derived fields and metadata
    
    ### Data Storage Layer
    - **PostgreSQL Database**: Primary data store with optimized schemas
    - **Connection Pooling**: Efficient database connection management
    - **Indexing Strategy**: Performance-optimized database indexes
    - **Backup & Recovery**: Automated backup and disaster recovery
    
    ### Analytics Layer
    - **Real-time Analytics**: Continuous calculation of KPIs
    - **Performance Metrics**: On-time performance, delays, reliability
    - **Trend Analysis**: Historical data analysis and forecasting
    - **Anomaly Detection**: Automatic detection of service issues
    """)
    
    # Infrastructure
    st.markdown("""
    ## Infrastructure & Deployment
    
    ### Production Environment
    - **Operating System**: Ubuntu 20.04 LTS or later
    - **Python Version**: Python 3.8+
    - **Database**: PostgreSQL 12+
    - **Web Server**: Nginx for reverse proxy
    - **Process Management**: Systemd for service management
    
    ### Scalability Features
    - **Horizontal Scaling**: Multiple pipeline instances
    - **Load Balancing**: Distribute load across instances
    - **Database Sharding**: Partition data by time/route
    - **Caching Layer**: Redis for frequently accessed data
    
    ### Monitoring & Observability
    - **Application Metrics**: Custom business metrics
    - **Infrastructure Metrics**: CPU, memory, disk, network
    - **Log Aggregation**: Centralized logging with ELK stack
    - **Alerting**: Proactive notification of issues
    """)
    
    # Security
    st.markdown("""
    ## Security & Compliance
    
    ### Data Security
    - **API Key Management**: Secure storage of MBTA API keys
    - **Database Security**: Encrypted connections and access control
    - **Network Security**: Firewall rules and VPN access
    - **Data Encryption**: At-rest and in-transit encryption
    
    ### Access Control
    - **User Authentication**: Secure login mechanisms
    - **Role-Based Access**: Different permission levels
    - **Audit Logging**: Track all data access and changes
    - **Session Management**: Secure session handling
    """)
    
    # Performance
    st.markdown("""
    ## Performance Characteristics
    
    ### Data Processing
    - **Ingestion Rate**: 1000+ API calls per minute
    - **Processing Latency**: < 1 second end-to-end
    - **Data Throughput**: 10,000+ records per minute
    - **Storage Efficiency**: Optimized for read-heavy workloads
    
    ### Dashboard Performance
    - **Page Load Time**: < 2 seconds
    - **Query Response**: < 500ms for standard queries
    - **Real-time Updates**: 15-second refresh intervals
    - **Concurrent Users**: Support for 100+ simultaneous users
    """)

# Run the dashboard
if __name__ == "__main__":
    main()
